---
description: 
globs: 
alwaysApply: true
---
# n8n Workflow Debugging Methodology

This rule establishes the systematic 5-phase debugging approach for n8n workflows, based on real-world patterns and solutions documented in [N8N_WORKFLOW_DEBUGGING.md](mdc:docs/N8N_WORKFLOW_DEBUGGING.md).

## **Core Debugging Philosophy**

- **Field Name Consistency**: The #1 cause of n8n workflow failures
- **Progressive Testing**: Individual nodes â†’ connections â†’ end-to-end
- **Error Pattern Recognition**: HTTP 500 errors have predictable root causes
- **Graceful Degradation**: Always include comprehensive error handling
- **Visual vs Structural Validation**: n8n UI display issues don't indicate actual connection problems

## **Critical Discovery: n8n Visual Linking vs JSON Structure**

### **âœ… ESTABLISHED PATTERN: Visual Display Issues Are Cosmetic**
Based on comprehensive testing (January 2025), when users report "nodes are unlinked" in n8n:

**Root Cause Analysis:**
- **Visual Issue**: n8n UI sometimes doesn't render connections properly
- **Structural Reality**: JSON connections are properly defined using node IDs
- **False Alarm**: Workflow functionality remains intact despite visual appearance

**Diagnostic Approach:**
```javascript
// âœ… DO: Create automated connection validation
// Use comprehensive testing script to verify:
// 1. All nodes have proper IDs
// 2. Connections reference correct node IDs (not names)
// 3. Execution flow traces correctly from trigger to completion
// 4. No actual structural issues exist

// âŒ DON'T: Assume visual = structural problem
// Don't rebuild workflows based on UI display alone
// Don't panic when connections appear unlinked visually
```

**Validation Pattern:**
```bash
# Systematic connection analysis using automated tools
node scripts/test-workflow-execution.js

# Expected output for healthy workflow:
# âœ… All connections are valid!
# ðŸ“Š Connection Summary: X connections, 0 issues
# ðŸš€ Status: Ready for execution
```

### **âœ… COMPREHENSIVE TESTING FRAMEWORK PATTERN**
**Established Testing Methodology:**
1. **Automated Workflow Tester** (`scripts/test-workflow-execution.js`)
   - Node structure analysis with type and ID mapping
   - Connection validation using proper node references
   - Execution flow tracing from trigger to completion
   - AI service integration endpoint testing

2. **AI Feature Demo Script** (`scripts/demo-ai-workflow.sh`)
   - Error analysis demonstrations (rate limiting, auth, data processing)
   - Feature showcase with real examples
   - Integration validation for all localhost service endpoints

**Implementation Pattern:**
```javascript
// âœ… DO: Systematic workflow analysis
const nodeMap = new Map();
workflow.nodes.forEach(node => nodeMap.set(node.id, node));

// Validate connections use node IDs, not names
connections.main?.forEach(connectionGroup => {
    connectionGroup.forEach(connection => {
        const targetNode = nodeMap.get(connection.node);
        if (!targetNode) {
            connectionIssues.push(`âŒ Target node not found: ${connection.node}`);
        }
    });
});
```

## **5-Phase Debugging Methodology**

### **Phase 1: IDENTIFY**
- **Automated Analysis**: Run `WorkflowValidator`, `DataFlowValidator`, and `ConnectionValidator` from `src/validation` to automatically detect common issues.
- **Error Classification**: Use `ErrorClassifier` from `src/error-handling` to understand the root cause of any exceptions.
- **Visual vs Structural**: Use automated testing scripts to distinguish UI display issues from actual connection problems.
- **HTTP 500 Errors**: Check trigger configuration, field mappings, HTTP parameters.
- **Node Failures**: Verify node-specific requirements (credentials, parameters, data types).
- **Data Flow Issues**: Trace data transformation between connected nodes.
- **Field Consistency**: Look for mismatched field names across the workflow.

### **Phase 2: ISOLATE**
- **Node Analysis**: Use `NodeAnalyzer` from `src/utils` to identify the specific node and its dependencies that are causing the failure.
- **Connection Verification**: Create comprehensive test scripts to validate actual JSON structure vs visual display.
- **Targeted Testing**: Test the isolated component with the `RealWorldTestingFramework` using sample data that replicates the error.
```javascript
// âœ… DO: Test individual components with automated tools
// Use NodeAnalyzer to find the problematic node
// Use RealWorldTestingFramework to test it with specific data
// Use automated connection validation scripts

// âŒ DON'T: Test entire workflow at once
// Skip individual node validation
// Assume data flow without verification
// Trust visual display over JSON structure analysis
```

### **Phase 3: FIX**
**Critical Fix Patterns:**
- **Automated Generation**: Use `WorkflowGenerator` from `src/generators` to rebuild faulty nodes or connections according to best practices.
- **Visual Linking Issues**: Refresh workflow in n8n or re-import JSON if connections appear unlinked but JSON structure is valid.
- **Schedule Triggers**: Always include complete cron configuration.
- **HTTP Requests**: Specify method, headers, and parameter structure.
- **Field References**: Use consistent naming (customer_id vs mixpanel_customer_id).
- **Data Transformations**: Handle undefined/null values gracefully, preferably with logic from our `ai-agents`.

### **Phase 4: VERIFY**
- **Automated Validation**: Re-run all validators from `src/validation` to confirm the fix has not introduced new issues.
- **Comprehensive Testing**: Use automated testing scripts to verify all connections and execution flow.
- **Individual Node Testing**: Each node executes successfully in isolation.
- **Connection Testing**: Data flows correctly between connected nodes.
- **End-to-End Testing**: Use the `RealWorldTestingFramework` to execute the complete workflow with the original problematic data.
- **Error Scenarios**: Use the `ErrorTestingFramework` to test for graceful handling of edge cases and failures.
- **AI Service Integration**: Test all localhost service endpoints for proper integration.

### **Phase 5: DOCUMENT**
- **Update Task Master**: Log findings using `update_subtask` with timestamps, including the output from our analysis tools.
- **Pattern Documentation**: Record successful patterns and fixes in `docs/LEARNINGS.md` for future reference.
- **Error Solutions**: Document specific fixes for common error patterns.
- **Testing Framework**: Document automated testing scripts and their usage patterns.
- **Visual vs Structural**: Document the distinction between UI display issues and actual problems.

## **AI Service Integration Testing Patterns**

### **âœ… ESTABLISHED AI INTEGRATION VALIDATION**
**Service Endpoint Testing:**
```bash
# Test AI service availability and functionality
curl -X POST http://localhost:3000/summarize-error \
  -H "Content-Type: application/json" \
  -d '{"error":"Test error","workflow":"test","timestamp":"2025-01-23T08:32:00Z"}'

# Expected: Intelligent error analysis response
# Validates: AI service integration working properly
```

**Integration Points to Test:**
- `/summarize-error` - Intelligent error analysis
- `/analyze-engagement` - Real-time engagement scoring  
- `/report-error` - Enhanced error reporting
- `/log-execution` - Comprehensive execution metrics

### **âœ… COMPREHENSIVE DEMO FRAMEWORK PATTERN**
**Automated Demo Script Structure:**
```bash
#!/bin/bash
# 1. Service status verification
# 2. Error analysis demonstrations (rate limiting, auth, data processing)
# 3. Feature showcase with real examples
# 4. Integration point validation
# 5. Comprehensive status reporting
```

## **Field Name Consistency Patterns**

### **âœ… DO: Consistent Field Naming**
```json
{
  "customer_id": "12345",           // Use throughout workflow
  "mixpanel_customer_id": "12345",  // Explicit mapping when needed
  "contact_id": "67890"             // Clear, consistent naming
}
```

### **âŒ DON'T: Inconsistent Field References**
```json
{
  "customerId": "12345",      // CamelCase
  "customer_id": "12345",     // Snake_case  
  "CustomerID": "12345"       // PascalCase
}
```

## **HTTP Node Configuration Patterns**

### **âœ… DO: Complete HTTP Configuration**
```json
{
  "method": "GET",
  "url": "https://api.example.com/endpoint",
  "headers": {
    "Authorization": "Bearer {{$credentials.token}}",
    "Content-Type": "application/json"
  },
  "qs": {
    "param1": "{{$json.field1}}",
    "param2": "{{$json.field2}}"
  }
}
```

### **âŒ DON'T: Incomplete HTTP Configuration**
```json
{
  "url": "https://api.example.com/endpoint"
  // Missing method, headers, parameters
}
```

## **Error Handling Patterns**

### **âœ… DO: Comprehensive Error Handling**
```javascript
// In Function nodes
try {
  const result = items.map(item => {
    const data = item.json;
    
    // Validate required fields
    if (!data.customer_id) {
      return { 
        error: 'Missing customer_id',
        original_data: data 
      };
    }
    
    // Process data
    return { 
      processed: true,
      customer_id: data.customer_id,
      result: processData(data)
    };
  });
  
  return result;
} catch (error) {
  return [{ 
    error: error.message,
    timestamp: new Date().toISOString()
  }];
}
```

## **Task Master Integration**

When debugging n8n workflows, use Task Master systematically:

```bash
# Log debugging progress
mcp_task-master-ai_update_subtask --id=<subtask-id> --prompt="
Phase 1 IDENTIFY: Found HTTP 500 error in HubSpot node
- **WorkflowValidator Output**: [Paste output here]
- **ErrorClassifier Output**: [Paste output here]
- **Automated Testing Results**: [Paste connection validation results]
- Root cause: Missing customer_id field mapping
- Affected nodes: HubSpot Contact Fetch, Mixpanel Update
- Error pattern: Field name inconsistency
- Visual vs Structural: JSON structure valid, UI display issue only
"

# Mark phases complete
mcp_task-master-ai_set_task_status --id=<subtask-id> --status=done
```

## **Quality Gates**

Before marking any n8n workflow task complete:
- [ ] All validators from `src/validation` pass
- [ ] Automated testing scripts confirm connection validity
- [ ] All HTTP 500 errors resolved
- [ ] Field name consistency verified
- [ ] Error handling implemented using `src/error-handling` tools
- [ ] End-to-end testing passed with `RealWorldTestingFramework`
- [ ] AI service integration tested and validated
- [ ] Visual vs structural issues properly diagnosed
- [ ] Documentation updated with testing results
- [ ] Task Master progress logged with tool outputs

## **Emergency Diagnostic Checklist**

When users report "unlinked nodes" or connection issues:
1. **âœ… FIRST**: Run automated testing script (`scripts/test-workflow-execution.js`)
2. **âœ… VERIFY**: Check JSON structure for proper node ID references
3. **âœ… VALIDATE**: Confirm execution flow traces correctly
4. **âœ… TEST**: Verify AI service integration endpoints
5. **âœ… DOCUMENT**: Log findings in Task Master with test results
6. **âœ… REASSURE**: Explain visual vs structural distinction to user

## **References**

- [N8N_WORKFLOW_DEBUGGING.md](mdc:docs/N8N_WORKFLOW_DEBUGGING.md) - Complete debugging guide
- [AI_AGENT_COLLABORATION_GUIDE.md](mdc:docs/AI_AGENT_COLLABORATION_GUIDE.md) - Tool usage patterns
- [workflow-linking-and-testing-demo.md](mdc:docs/workflow-linking-and-testing-demo.md) - Comprehensive testing results

---

*This systematic approach ensures consistent, high-quality n8n workflow development and maintenance, with special attention to distinguishing visual display issues from actual structural problems.*
